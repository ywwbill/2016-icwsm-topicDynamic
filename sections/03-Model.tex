\section{Single Topic LDA}
\label{sec:model}

\subsection{Graphical Model and Generative Process}

In this paper, we propose \stlda to deal with short documents like tweets. The intuition is that short documents are unlikely to be related to multiple topics. Instead, one short document is usually talking about a single topic and all its words are generated from this topic. Specifically, the generative process of tweets is defined as

\begin{enumerate}
\item For each topic $k \in \{1, \ldots, K\}$
    \begin{enumerate}
    \item Draw word distribution $\bm{\phi_k} \sim \mathrm{Dir}(\beta)$
    \end{enumerate}
\item Draw topic distribution $\bm{\theta} \sim \mathrm{Dir}(\alpha)$
\item For each document $d \in \{1, \ldots, D\}$
    \begin{enumerate}
    \item Draw a topic $z_d \sim \mathrm{Mult}(\bm{\theta})$
    \item For each token $t_{d,n}$ in document $d$
        \begin{enumerate}
        \item Draw a word $w_{d,n} \sim \mathrm{Mult}(\bm{\phi_{z_d}})$
        \end{enumerate}
    \end{enumerate}
\end{enumerate}

The corresponding graphical model is given in Figure~\ref{fig:stlda}.

\begin{figure}[h]
\centering
\includegraphics[width=.8\linewidth]{figures/single.pdf}
\caption{Graphical Model of \stlda}\label{fig:stlda}
\end{figure}

The model looks similar to conventional LDA, since they have same nodes. However, the coverage of two plates are different, which is adapted according to our assumptions.

The first difference is the coverage of word plate (the one with subscript $N_d$). In LDA, it covers both $\bm{z}$ and $\bm{w}$, denoting that every word has its own topic assignment and every document consists of a mixture of topics. In \stlda, the plate only covers $\bm{w}$, which means that every word in a tweet is generated from the same topic.

The second difference is the coverage of document plate (the one with subscript $D$). LDA assumes that every document has a topic distribution, but in \stlda, every document only has one topic which can not form a distribution inside a document. Thus, $\bm{\theta}$ is outside the document plate and denotes a background topic distribution of the corpus.

\subsection{Posterior Inference}

The posterior inference of \stlda can be done by Gibbs sampling. The probability of document $d$ being assigned a topic $k$ is computed as

\begin{align}
&\prob {z_d=k} {\bm{z_{-d}},\bm{w}} \notag\\
\propto &\left( N_k^{-d} + \alpha \right) \frac {\prod\limits_{v=1}^{V} \prod\limits_{i=0}^{N_{d,v}-1} \left( N_{k,v}^{-d} + \beta + i \right)} {\prod\limits_{i=0}^{N_{d,\cdot}-1} \left( N_{k,\cdot}^{-d} + V\beta + i \right)},
\end{align}
where $N_k$ denotes the number of documents assigned to topic $k$; $N_{d,v}$ is the count of word $v$ in document $d$; $N_{k,v}$ denotes the count of word $v$ assigned to topic $k$. Marginal counts are denoted by $\cdot$. $^{-d}$ denotes that the count excludes document $d$.

A brief derivation of Gibbs sampling equation is attached in Appendix~\ref{sec:derivation}.

% After we train a model on training corpus, we can apply it on unseen documents and infer their topic assignments. The Gibbs sampling equation for test is much simpler. The probability of assigning topic $k$ to document $d$ is

% \begin{equation}
% \prob {z_d=k} {\bm{z_{-d}},\bm{w_d}} \propto \left( N_k^{-d} + \alpha \right) \prod_{v=1}^{V} \phi_{k,v}^{N_{d,v}}.
% \end{equation}

